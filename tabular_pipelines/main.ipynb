{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Fast and Efficient Tabular Data Pipelines "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually, tabular data processing does not leave a Jupyter notebook as it is more convenient for data exploration and visualization. However, when we are handling large amounts of data, we are met with computational and memory constraints that affect the entire pipeline speed and our budget. Buying more memory and faster CPUs with more cores is not a sustainable solution and in some settings may not be available or help at all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problems\n",
    "\n",
    "- Data does not fit memory e.g. cancer data\n",
    "    - The main dataset used by lots of researchers is the **TCGA** dataset\n",
    "    - It contains ~3 million rows and ~120 columns, this is multiple gigabytes\n",
    "    - Loading this data using `pandas.read_csv(file_path)` on a VM on Azure with **16 GB** memory and **4 cores** crashes the VM due to insufficient memory\n",
    "\n",
    "- Running processing steps like filtration and grouping may create necessary copies which slows down the pipeline\n",
    "\n",
    "- Running algorithms like clustering patients or tumors can take too long on data this big"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Possible Solutions\n",
    "\n",
    "- Buy larger memory and faster CPU with more cores. **Costs money!**\n",
    "- Use CPU cores to parallelize the pipeline. **Not always possible!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by creating a dummy dataset of 3 million rows and 102 columns of different datatypes like string, dates, integers and floats and save it into a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "dtypes = [np.int64, np.float64, str]\n",
    "nrows, ncols = 3000000,100\n",
    "\n",
    "float_rows = np.random.rand(nrows, ncols//2)\n",
    "int_rows = (np.random.rand(nrows, ncols//2) * np.random.randint(0,100)).astype(np.int32)\n",
    "string_row = np.expand_dims(np.array([f\"TCGA_{np.random.randint(0,100)}\" for _ in range(nrows)]),1)\n",
    "date_row = np.expand_dims(np.array([f\"{np.random.randint(1980,2000)}-{np.random.randint(1,13)}-{np.random.randint(1,28)}\" for _ in range(nrows)]),1)\n",
    "\n",
    "rows = np.hstack([string_row,date_row,int_rows,float_rows])\n",
    "\n",
    "cols = [f\"col_{i}\" for i in range(ncols+2)]\n",
    "\n",
    "df = pd.DataFrame(rows, columns=cols)\n",
    "df.to_csv(\"dummy_data.csv\",index=False)\n",
    "df.head(5)\n",
    "del df, rows, int_rows, float_rows, string_row, date_row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After saving the file, we start reading it using pandas, nothing fancy just our usual `read_csv()` function. Then, we see the memory usage of each column in the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read file in 2.8e+01 s\n",
      "Total memory usage: 2789 MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index            128\n",
       "col_0      191699718\n",
       "col_1      197751198\n",
       "col_2       24000000\n",
       "col_3       24000000\n",
       "             ...    \n",
       "col_97      24000000\n",
       "col_98      24000000\n",
       "col_99      24000000\n",
       "col_100     24000000\n",
       "col_101     24000000\n",
       "Length: 103, dtype: int64"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = time.perf_counter()\n",
    "df = pd.read_csv(\"dummy_data.csv\")\n",
    "end = time.perf_counter()\n",
    "print(f\"Read file in {end-start:.2} s\")\n",
    "print(f\"Total memory usage: {df.memory_usage(deep=True).sum() // 1000000} MB\")\n",
    "df.memory_usage(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "col_0       object\n",
       "col_1       object\n",
       "col_2        int64\n",
       "col_3        int64\n",
       "col_4        int64\n",
       "            ...   \n",
       "col_97     float64\n",
       "col_98     float64\n",
       "col_99     float64\n",
       "col_100    float64\n",
       "col_101    float64\n",
       "Length: 102, dtype: object"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see above that the dataframe takes about 1300 MB of memory. We can see that `col_1` was parsed as a generic object not a date. Also, we can see that integers and floats were read in **64 bit** precision. A simple optimization would be to specify column datatypes to `read_csv()` and force pandas to parse the date, and reduce the precision of numeric values. This small addition led to reduction in memory usage by **50%**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read file in 1.6e+01 s\n",
      "Total memory usage: 1415 MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index            128\n",
       "col_0      191699718\n",
       "col_1       24000000\n",
       "col_2       12000000\n",
       "col_3       12000000\n",
       "             ...    \n",
       "col_97      12000000\n",
       "col_98      12000000\n",
       "col_99      12000000\n",
       "col_100     12000000\n",
       "col_101     12000000\n",
       "Length: 103, dtype: int64"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtypes = dict(zip(cols[2:],[np.int32]*50+[np.float32]*50))\n",
    "start = time.perf_counter()\n",
    "df = pd.read_csv(\"dummy_data.csv\", sep=\",\",dtype=dtypes,parse_dates=[\"col_1\"])\n",
    "end = time.perf_counter()\n",
    "print(f\"Read file in {end-start:.2} s\")\n",
    "print(f\"Total memory usage: {df.memory_usage(deep=True).sum() // 1000000} MB\")\n",
    "df.memory_usage(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "col_0              object\n",
       "col_1      datetime64[ns]\n",
       "col_2               int32\n",
       "col_3               int32\n",
       "col_4               int32\n",
       "                ...      \n",
       "col_97            float32\n",
       "col_98            float32\n",
       "col_99            float32\n",
       "col_100           float32\n",
       "col_101           float32\n",
       "Length: 102, dtype: object"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In larger datasets, this can lead to a reduction of GBs of memory and makes computation possible in low memory environments where previously we would have had to upgrade the memory. However, there are situations where our dataset is very large and setting datatypes did not help us in reading the file. One thing we could do is read the data in chunks and do some processing on these chunks if possible, then save the result on disk. This results in reduced memory usage of **1 GB** and saving time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The memory_profiler extension is already loaded. To reload it, use:\n",
      "  %reload_ext memory_profiler\n"
     ]
    }
   ],
   "source": [
    "%load_ext memory_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process():\n",
    "    start = time.perf_counter()\n",
    "\n",
    "    df = pd.read_csv(\"dummy_data.csv\", header=0, dtype=dtypes, parse_dates=[\"col_1\"])\n",
    "\n",
    "    # filtration\n",
    "    before_date = df[df[\"col_1\"] <= \"1990-12-24\"]\n",
    "\n",
    "    col2_gt_0 = before_date[before_date[\"col_2\"] > 0]\n",
    "\n",
    "    col3_gt_50 = col2_gt_0[col2_gt_0[\"col_3\"] > 50]\n",
    "\n",
    "    col_100_lt_05 = col3_gt_50[col3_gt_50[\"col_100\"] < 0.5]\n",
    "        \n",
    "    col_100_lt_05.to_csv(\"processed_chunks.csv\", sep=\",\", mode=\"a\", header=False, index=False,columns=cols)\n",
    "\n",
    "    end = time.perf_counter()\n",
    "    print(f\"Processed data in {end-start:.2} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed data in 1.6e+01 s\n",
      "peak memory: 6033.54 MiB, increment: 1144.41 MiB\n"
     ]
    }
   ],
   "source": [
    "%memit process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process():\n",
    "    start = time.perf_counter()\n",
    "\n",
    "    for i,chunk in enumerate(pd.read_csv(\"dummy_data.csv\", header=0, dtype=dtypes, parse_dates=[\"col_1\"], chunksize=500000)):\n",
    "\n",
    "        # filtration\n",
    "        before_date = chunk[chunk[\"col_1\"] <= \"1990-12-24\"]\n",
    "\n",
    "        col2_gt_0 = before_date[before_date[\"col_2\"] > 0]\n",
    "        \n",
    "        col3_gt_50 = col2_gt_0[col2_gt_0[\"col_3\"] > 50]\n",
    "        \n",
    "        col_100_lt_05 = col3_gt_50[col3_gt_50[\"col_100\"] < 0.5]\n",
    "            \n",
    "        col_100_lt_05.to_csv(\"processed_chunks.csv\", sep=\",\", mode=\"a\", header=False, index=False,columns=cols)\n",
    "\n",
    "    end = time.perf_counter()\n",
    "    print(f\"Processed data in {end-start:.2} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed data in 1.6e+01 s\n",
      "peak memory: 4697.96 MiB, increment: 0.00 MiB\n"
     ]
    }
   ],
   "source": [
    "%memit process()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chunking the data enables us to use multiprocessing to process chunks in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed chunk 0 in 0.06 s, process #: 689143\n",
      "Processed chunk 1 in 0.058 s, process #: 689144\n",
      "Processed chunk 2 in 0.049 s, process #: 689145\n",
      "Processed chunk 3 in 0.04 s, process #: 689143\n",
      "Processed chunk 4 in 0.04 s, process #: 689144\n",
      "Processed chunk 5 in 0.039 s, process #: 689145\n",
      "Processed chunks in 1.7e+01 s, process #: 676661\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import Pool, Lock\n",
    "import os\n",
    "\n",
    "l = Lock()\n",
    "\n",
    "def process(chunk,i):\n",
    "    start = time.perf_counter()\n",
    "\n",
    "    # filtration\n",
    "    before_date = chunk[chunk[\"col_1\"] <= \"1990-12-24\"]\n",
    "\n",
    "    col2_gt_0 = before_date[before_date[\"col_2\"] > 0]\n",
    "\n",
    "    col3_gt_50 = col2_gt_0[col2_gt_0[\"col_3\"] > 50]\n",
    "\n",
    "    col_100_lt_05 = col3_gt_50[col3_gt_50[\"col_100\"] < 0.5]\n",
    "    \n",
    "    l.acquire()\n",
    "    col_100_lt_05.to_csv(f\"processed_chunks.csv\", sep=\",\", mode=\"a\", header=False, index=False,columns=cols)\n",
    "    l.release()\n",
    "\n",
    "    end = time.perf_counter()\n",
    "    print(f\"Processed chunk {i} in {end-start:.2} s, process #: {os.getpid()}\")\n",
    "\n",
    "pool = Pool(processes=3)\n",
    "\n",
    "\n",
    "iterator = pd.read_csv(\"dummy_data.csv\", header=0, dtype=dtypes, parse_dates=[\"col_1\"], chunksize=500000)\n",
    "\n",
    "start = time.perf_counter()\n",
    "for i,chunk in enumerate(iterator):\n",
    "    pool.apply_async(process, (chunk,i,))\n",
    "\n",
    "pool.close()\n",
    "pool.join()\n",
    "\n",
    "end = time.perf_counter()\n",
    "print(f\"Processed chunks in {end-start:.2} s, process #: {os.getpid()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. https://pandas.pydata.org/docs/user_guide/scale.html\n",
    "2. https://pypi.org/project/memory-profiler/\n",
    "3. https://youtu.be/di-cXkg-hJU?si=VFL6r1PcHW-HGEQw\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
